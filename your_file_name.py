# -*- coding: utf-8 -*-
import os
import re
import json
import difflib
from datetime import datetime
import pytesseract
from PIL import ImageGrab
import pyautogui
import psutil
from transformers import pipeline, AutoModelForSeq2SeqLM, AutoTokenizer

# ğŸ§  æŒ‡å®š Tesseract çš„åŸ·è¡Œæª”è·¯å¾‘ï¼ˆè«‹ç¢ºèªä½ çš„å¯¦éš›å®‰è£è·¯å¾‘ï¼‰
pytesseract.pytesseract.tesseract_cmd = r"C:\Program Files\Tesseract-OCR\tesseract.exe"

class SmartAssistant:
    def __init__(self):
        self.memory_path = os.path.expanduser("~/SmartAssistant")
        os.makedirs(self.memory_path, exist_ok=True)

        self._load_models()
        self.long_memory = self._load_json("memory.json", default={"conversations":[]})
        self.skill_lib = self._load_json("skills.json", default={
            "time": {
                "patterns": ["ç°åœ¨å‡ ç‚¹", "å½“å‰æ—¶é—´", "å‡ ç‚¹é’Ÿäº†"],
                "action": lambda: f"ğŸ•’ ç°åœ¨æ˜¯ {datetime.now().strftime('%H:%M')}"
            },
            "date": {
                "patterns": ["ä»Šå¤©å‡ å·", "ç°åœ¨æ—¥æœŸ", "ä»Šå¤©æ˜¯"],
                "action": lambda: f"ğŸ“… ä»Šå¤©æ˜¯ {datetime.now().strftime('%Yå¹´%mæœˆ%dæ—¥')}"
            }
        })

        print("ğŸ’¡ æ™ºèƒ½åŠ©ç†å·²å°±ç»ªï¼å°è¯•è¯´ï¼š'å†™ä¸ªå€’è®¡æ—¶ç¨‹åº' æˆ– 'æˆªå›¾ä¿å­˜åˆ°æ¡Œé¢'")

    def _load_models(self):
        self.nlp_tokenizer = AutoTokenizer.from_pretrained("uer/t5-small-chinese-cluecorpussmall")
        self.nlp_model = AutoModelForSeq2SeqLM.from_pretrained("uer/t5-small-chinese-cluecorpussmall")
        self.pattern_engine = {
            'code': ["å†™ä»£ç ", "ç¼–ç¨‹", "å†™ä¸ªç¨‹åº"],
            'file': ["æ‰“å¼€æ–‡ä»¶", "æŸ¥çœ‹æ–‡æ¡£", "æ‰¾ä¸€ä¸‹"],
            'media': ["æ’­æ”¾", "æš‚åœ", "éŸ³é‡"]
        }

    def _load_json(self, filename, default=None):
        path = os.path.join(self.memory_path, filename)
        if os.path.exists(path):
            with open(path, 'r', encoding='utf-8') as f:
                return json.load(f)
        return default if default else {}

    def _save_json(self, filename, data):
        with open(os.path.join(self.memory_path, filename), 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)

    def _understand(self, text):
        for skill, data in self.skill_lib.items():
            if any(patt in text for patt in data["patterns"]):
                return skill

        for category, patterns in self.pattern_engine.items():
            if any(patt in text for patt in patterns):
                return category

        closest = difflib.get_close_matches(text, [p for skill in self.skill_lib.values() for p in skill["patterns"]], n=1)
        if closest:
            return next((k for k,v in self.skill_lib.items() if closest[0] in v["patterns"]), None)

        inputs = self.nlp_tokenizer(f"ç†è§£æ„å›¾: {text}", return_tensors="pt")
        outputs = self.nlp_model.generate(inputs.input_ids)
        return self.nlp_tokenizer.decode(outputs[0], skip_special_tokens=True)

    def _execute(self, intent, text):
        builtin = {
            'code': lambda: self._generate_code(text),
            'screenshot': lambda: self._take_screenshot(text),
            'search': lambda: self._search_memory(text)
        }

        if intent in self.skill_lib:
            return self.skill_lib[intent]["action"]()

        if intent in builtin:
            return builtin[intent]()

        return "ğŸ¤” æˆ‘ä¸å¤ªç¡®å®šæ‚¨çš„æ„æ€ï¼Œè¯·æ¢ç§è¯´æ³•è¯•è¯•ï¼Ÿ"

    def _generate_code(self, request):
        lang = "Python" if "python" in request.lower() else ""
        prompt = f"ç”¨{lang}å†™ä¸€ä¸ª{request.replace('å†™', '').replace('ä»£ç ', '')}"

        samples = {
            "å€’è®¡æ—¶": "import time\nfor i in range(5,0,-1):\n    print(i); time.sleep(1)",
            "è®¡ç®—å™¨": "result = eval(input('è¾“å…¥ç®—å¼: '))\nprint(f'ç»“æœæ˜¯: {result}')"
        }

        closest = difflib.get_close_matches(request, samples.keys(), n=1)
        return f"```{lang}\n{samples[closest[0]] if closest else '# ä»£ç ç”ŸæˆåŠŸèƒ½å¾…æ‰©å±•'}\n```"

    def _take_screenshot(self, text):
        path = os.path.join(os.path.expanduser("~"), "Desktop/screenshot.png")
        ImageGrab.grab().save(path)
        return f"ğŸ“¸ æˆªå›¾å·²ä¿å­˜åˆ°æ¡Œé¢ (è·¯å¾„: {path})"

    def process(self, text):
        if text.startswith("å­¦ä¹ "):
            try:
                _, pattern, action = text.split("|")
                new_skill = {
                    "patterns": [pattern.strip()],
                    "action": eval(f"lambda: {action.strip()}")
                }
                self.skill_lib[f"custom_{len(self.skill_lib)}"] = new_skill
                self._save_json("skills.json", self.skill_lib)
                return f"ğŸ¯ å·²å­¦ä¼š: å½“ä½ è¯´'{pattern}'æ—¶ï¼Œæˆ‘ä¼šæ‰§è¡Œ: {action}"
            except:
                return "å­¦ä¹ æ ¼å¼: å­¦ä¹ |è§¦å‘è¯|æ‰§è¡ŒåŠ¨ä½œ"

        intent = self._understand(text)
        response = self._execute(intent, text)

        self.long_memory["conversations"].append({
            "time": datetime.now().isoformat(),
            "input": text,
            "intent": intent,
            "response": response
        })

        return response

if __name__ == "__main__":
    ai = SmartAssistant()
    print("""\nğŸ’¬ äº¤äº’æŒ‡å—:
1. ç›´æ¥è¯´éœ€æ±‚ (ä¾‹: "è®¾ç½®5ç‚¹æé†’")
2. æ•™æ–°æŠ€èƒ½ (ä¾‹: "å­¦ä¹ |æ‰“å¼€éŸ³ä¹|os.system('start spotify')")
3. è¾“å…¥ exit é€€å‡º""")
    
    while True:
        try:
            user_input = input("\næ‚¨: ").strip()
            if user_input.lower() in ["exit", "é€€å‡º"]:
                ai._save_json("memory.json", ai.long_memory)
                print("ğŸ›‘ åŠ©ç†å·²é€€å‡º")
                break
                
            if user_input:
                response = ai.process(user_input)
                print(f"åŠ©ç†: {response}")
                
        except KeyboardInterrupt:
            ai._save_json("memory.json", ai.long_memory)
            print("\nâš ï¸ å¯¹è¯å·²ä¿å­˜")
            break
